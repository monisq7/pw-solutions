{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 What is a parameter?\n",
        "\n",
        "\n",
        "Answer 1\n",
        "In Machine Learning, a parameter is an internal variable of the model that the algorithm learns from the training data. They are not set by the user but are estimated during the training process.\n",
        "\n",
        "Examples:\n",
        "\n",
        "In a Linear Regression model (y = mx + c), the coefficients (m) and the intercept (c) are the parameters.\n",
        "\n",
        "In a Neural Network, the weights and biases connecting the neurons are the parameters.\n",
        "\n",
        "Key Point: Parameters define the model's learned behavior and are used to make predictions.\n",
        "\n",
        "Do not confuse this with a hyperparameter, which is a configuration variable set by the user before training (e.g., learning rate, number of trees in a Random Forest)."
      ],
      "metadata": {
        "id": "3TTyIoLNjokd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 What is correlation? What does negative correlation mean?\n",
        "\n",
        "Answer 2\n",
        "Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables.\n",
        "\n",
        "It is quantified by the correlation coefficient, which ranges from -1 to +1.\n",
        "\n",
        "+1: Perfect positive linear relationship (as one variable increases, the other increases proportionally).\n",
        "\n",
        "0: No linear relationship between the variables.\n",
        "\n",
        "-1: Perfect negative linear relationship (as one variable increases, the other decreases proportionally).\n",
        "\n",
        "\n",
        "Negative correlation means that as one variable increases, the other variable tends to decrease. It represents an inverse relationship.\n",
        "\n",
        "Example: The amount of time you spend practicing a sport and your number of errors in a game. As practice time (Variable A) increases, errors (Variable B) typically decrease. This would be represented by a correlation coefficient between 0 and -1."
      ],
      "metadata": {
        "id": "f6Syn765kvHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Ans 3\n",
        "\n",
        "Definition: Machine Learning is a subset of Artificial Intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. The core idea is to learn from data.\n",
        "\n",
        "Main Components:\n",
        "\n",
        "Data: The foundation. Without data, there is nothing to learn from.\n",
        "\n",
        "Features: The individual measurable properties or characteristics of the data.\n",
        "\n",
        "Model / Algorithm: The mathematical function that learns the patterns from the features. (e.g., Linear Regression, Decision Tree).\n",
        "\n",
        "Loss Function / Cost Function: A function that measures how wrong the model's predictions are compared to the actual values. The goal of training is to minimize this function.\n",
        "\n",
        "Optimization Algorithm: The procedure used to adjust the model's parameters to minimize the loss function (e.g., Gradient Descent)."
      ],
      "metadata": {
        "id": "okqg5pWClPpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Ans4\n",
        "\n",
        "The loss value (or error) is a direct measure of the model's performance on a given dataset.\n",
        "\n",
        "Lower Loss: Indicates that the model's predictions are closer to the actual values, meaning the model is performing well on the data it's being evaluated on.\n",
        "\n",
        "Key Caveat: A very low loss on the training data but a high loss on new, unseen data (the test data) indicates overfitting. This means the model has memorized the training data instead of learning generalizable patterns.\n",
        "\n",
        "Therefore, the loss value is crucial for comparing models and diagnosing issues like overfitting or underfitting."
      ],
      "metadata": {
        "id": "pUV-o8W9l2lA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5 What are continuous and categorical variables?\n",
        "\n",
        "Ans5\n",
        "\n",
        "Continuous Variables: Can take on an infinite number of values within a given range. They are numeric and measurable.\n",
        "\n",
        "Examples: Height, Weight, Temperature, Time, Price.\n",
        "\n",
        "Categorical Variables: Represent types or categories. They take on a limited, fixed number of possible values.\n",
        "\n",
        "Examples: Gender (Male/Female/Other), Color (Red/Blue/Green), Country, Product Type."
      ],
      "metadata": {
        "id": "KGK2KgNVmCwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6 How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Ans 6\n",
        "Most ML algorithms require numerical input, so we must convert categorical variables into numbers. Common techniques include:\n",
        "\n",
        "Label Encoding: Assigns a unique integer to each category (e.g., \"Red\"=0, \"Blue\"=1, \"Green\"=2).\n",
        "\n",
        "Risk: Can imply an ordinal order (2 > 1 > 0) where none exists, which might mislead the model.\n",
        "\n",
        "One-Hot Encoding: Creates new binary (0/1) columns for each category. The original column is dropped.\n",
        "\n",
        "Example: A \"Color\" column becomes three columns: is_Red, is_Blue, is_Green.\n",
        "\n",
        "Use Case: Best for nominal data (no order).\n",
        "\n",
        "Target Encoding (Mean Encoding): Replaces each category with the average value of the target variable for that category.\n",
        "\n",
        "Example: For predicting house prices, you could replace the \"Neighborhood\" category with the average price of houses in that neighborhood."
      ],
      "metadata": {
        "id": "gJAqPxfxmZJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7 What do you mean by training and testing a dataset?\n",
        "\n",
        "Ans7\n",
        "This is the fundamental practice of evaluating a model's performance.\n",
        "\n",
        "Training Dataset: The subset of data used to train the model. The model learns patterns by adjusting its parameters based on this data.\n",
        "\n",
        "Testing Dataset: A separate, held-out subset of data used to test the model's performance after it has been trained. This data is completely unseen by the model during training and provides an unbiased evaluation of its ability to generalize to new data."
      ],
      "metadata": {
        "id": "3qlg0JNQmrzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8 What is sklearn.preprocessing?\n",
        "\n",
        "Ans8 sklearn.preprocessing is a module in the Scikit-Learn library (a popular Python ML library) that contains numerous utility functions and transformer classes for feature engineering and data preprocessing.\n",
        "\n",
        "Common tasks it handles:\n",
        "\n",
        "Scaling/Normalization: StandardScaler, MinMaxScaler\n",
        "\n",
        "Encoding Categorical Variables: OneHotEncoder, LabelEncoder\n",
        "\n",
        "Imputing Missing Values: SimpleImputer\n",
        "\n",
        "Creating Polynomial Features: PolynomialFeatures"
      ],
      "metadata": {
        "id": "yOTNk589m9b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9 What is a Test set?\n",
        "\n",
        "Ans 9 The Test Set is the portion of the original dataset that is strictly reserved for the final evaluation of the model. It must never be used during training or parameter tuning. Its sole purpose is to provide an unbiased estimate of the model's performance in a real-world scenario.\n",
        "\n"
      ],
      "metadata": {
        "id": "NyJRCia3nJG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10 How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "Ans 10 The most common way is using the train_test_split function from sklearn.model_selection.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X: Features, y: Target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "test_size=0.2: 20% of the data is used for testing, 80% for training.\n",
        "\n",
        "random_state=42: Ensures the split is reproducible.\n",
        "\n",
        "\n",
        "This is a structured, high-level approach:\n",
        "\n",
        "Problem Definition: Clearly define the goal. What are you trying to predict? What is the business objective?\n",
        "\n",
        "Data Collection: Gather the relevant data from various sources (databases, APIs, files).\n",
        "\n",
        "Data Preprocessing & Exploration (EDA):\n",
        "\n",
        "Handle missing values, duplicates, and errors.\n",
        "\n",
        "Perform Exploratory Data Analysis (EDA) to understand patterns, relationships, and distributions.\n",
        "\n",
        "Feature Engineering: Create new features, encode categorical variables, and scale numerical features as needed.\n",
        "\n",
        "Model Selection & Training:\n",
        "\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Choose one or more appropriate algorithms (e.g., Linear Regression, Random Forest).\n",
        "\n",
        "Train the model(s) on the training data.\n",
        "\n",
        "Model Evaluation:\n",
        "\n",
        "Use the trained model to make predictions on the test set.\n",
        "\n",
        "Evaluate performance using relevant metrics (e.g., Accuracy, Precision, Recall, Mean Squared Error).\n",
        "\n",
        "Model Tuning (Hyperparameter Optimization): Improve performance by tuning the model's hyperparameters (e.g., using Grid Search or Random Search).\n",
        "\n",
        "Interpretation & Deployment:\n",
        "\n",
        "Interpret the results and communicate findings to stakeholders.\n",
        "\n",
        "If satisfactory, deploy the model to a production environment for making predictions on new data.\n",
        "\n",
        "Monitoring & Maintenance: Continuously monitor the model's performance in the real world and retrain it with new data as needed (concept drift).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KWW5qd6knR_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11 Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Ans 11 Exploratory Data Analysis (EDA) is a critical first step. Fitting a model without EDA is like building a house without checking the foundation—it's likely to be unstable and flawed.\n",
        "\n",
        "Key Reasons for EDA:\n",
        "\n",
        "Understand Data Distribution: EDA helps you see if features are normally distributed, skewed, or have outliers. This informs decisions about data transformation (e.g., using log transformation for skewed data) and which models might be suitable.\n",
        "\n",
        "Identify Data Quality Issues: You can spot missing values, incorrect data types, duplicates, and errors that must be cleaned before modeling.\n",
        "\n",
        "Detect Outliers: Outliers can disproportionately influence many models (especially linear models). EDA helps you decide whether to remove, cap, or treat them.\n",
        "\n",
        "Uncover Relationships: You can see how features relate to each other (correlation) and to the target variable. This is vital for feature engineering (creating new features) and feature selection (removing redundant features).\n",
        "\n",
        "Validate Assumptions: Many algorithms have underlying assumptions (e.g., linearity, independence). EDA helps you check if these assumptions hold.\n",
        "\n",
        "Guide Modeling Strategy: The insights from EDA directly influence your choice of model, preprocessing steps, and overall approach.\n",
        "\n",
        "In short, EDA saves time, prevents garbage-in-garbage-out scenarios, and leads to more robust and accurate models."
      ],
      "metadata": {
        "id": "qFvCZHK7oDCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12 What is correlation?\n",
        "\n",
        "Ans 12\n",
        "Correlation: A statistical measure (between -1 and +1) of the strength and direction of a linear relationship between two variables."
      ],
      "metadata": {
        "id": "ogpEwQg0oo-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13  What does negative correlation mean?\n",
        "\n",
        "Ans 13 Negative Correlation: An inverse relationship (value between -1 and 0). As one variable increases, the other tends to decrease.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hq7Ztcw_o5_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14 How can you find correlation between variables in Python?\n",
        "\n",
        "Ans 14 The most common way is to use the .corr() method on a Pandas DataFrame. It calculates the Pearson correlation coefficient by default.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "# 1. Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        "\n",
        "# 2. Visualize with a heatmap (highly recommended for EDA)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# 3. To find correlation between two specific columns\n",
        "corr_value = df['Feature_A'].corr(df['Feature_B'])\n",
        "print(f\"Correlation between Feature_A and Feature_B: {corr_value:.2f}\")"
      ],
      "metadata": {
        "id": "KjzE_nuNpBwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15 What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Ans 15\n",
        "Correlation: Means two variables move together in a predictable way. It describes an association.\n",
        "\n",
        "Causation (Causality): Means a change in one variable directly brings about a change in another variable. It describes a cause-and-effect relationship.\n",
        "\n",
        "The Golden Rule: \"Correlation does not imply causation.\"\n",
        "\n",
        "Classic Example:\n",
        "\n",
        "Observation: There is a strong positive correlation between ice cream sales and the number of drownings.\n",
        "\n",
        "Correlation Interpretation: When ice cream sales are high, drownings are also high.\n",
        "\n",
        "Incorrect Causation (Post Hoc Fallacy): \"Eating ice cream causes people to drown.\"\n",
        "\n",
        "True Causation (Confounding Variable): The summer heat (the confounding variable) is the true cause. Hot weather causes both more people to buy ice cream and more people to go swimming, which leads to more drownings."
      ],
      "metadata": {
        "id": "LTz3FspcpZEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16 What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans 16 An Optimizer is an algorithm that adjusts a model's parameters (like weights and biases in a neural network) to minimize the loss function. It's the mechanism behind \"learning.\"\n",
        "\n",
        "Common Optimizers:\n",
        "\n",
        "Gradient Descent (GD): The fundamental optimizer.\n",
        "\n",
        "How it works: Calculates the gradient (direction of steepest ascent) of the loss function with respect to all parameters and then takes a step in the opposite direction (steepest descent). The size of the step is determined by the learning rate.\n",
        "\n",
        "Analogy: Finding the bottom of a valley by always walking downhill.\n",
        "\n",
        "Stochastic Gradient Descent (SGD):\n",
        "\n",
        "Difference from GD: Instead of using the entire dataset to calculate the gradient (which is slow), SGD uses one random training example at a time. This is much faster but \"noisier.\"\n",
        "\n",
        "Mini-batch Gradient Descent:\n",
        "\n",
        "A compromise: Uses a small random subset of the data (a mini-batch) for each step. This is the most common approach in practice, balancing speed and stability.\n",
        "\n",
        "Adam (Adaptive Moment Estimation): A very popular and effective optimizer.\n",
        "\n",
        "How it works: It combines ideas from two other optimizers (RMSprop and SGD with momentum). It adapts the learning rate for each parameter individually by using the moving averages of both the gradients (first moment) and the squared gradients (second moment)."
      ],
      "metadata": {
        "id": "o-z8lpnKpmYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17 What is sklearn.linear_model ?\n",
        "\n",
        "Ans 17\n",
        "sklearn.linear_model is a module in Scikit-Learn that contains a wide variety of linear models.\n",
        "\n",
        "Key Models it includes:\n",
        "\n",
        "LinearRegression: For standard regression tasks.\n",
        "\n",
        "LogisticRegression: Despite its name, it's primarily used for classification tasks.\n",
        "\n",
        "Ridge / Lasso / ElasticNet: These are linear models with regularization (L2, L1, or a mix) to prevent overfitting.\n",
        "\n",
        "SGDRegressor / SGDClassifier: Models that use Stochastic Gradient Descent for training, useful for very large datasets."
      ],
      "metadata": {
        "id": "PKb9leM7pw6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18 What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Ans 18 What it does: The fit() method is used to train the model. It finds the optimal parameters for the model based on the training data. This is where the \"learning\" happens.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "X: (Required) The feature matrix (training data). A 2D array-like structure (e.g., NumPy array or Pandas DataFrame) of shape (n_samples, n_features).\n",
        "\n",
        "y: (Required) The target vector. A 1D array-like structure of shape (n_samples,) containing the labels or values we want to predict.\n",
        "\n",
        "(Optional) Other arguments specific to the model, like sample_weight.\n",
        "\n",
        "Example: model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ENnjwWwDp8ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19 What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans 19\n",
        "What it does: The predict() method is used to make predictions using the trained model. It uses the parameters learned during fit() to generate outputs for new, unseen data.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "X: (Required) The feature matrix of the new data you want to predict. It must have the same number of features as the training data X used in fit(). Shape: (n_samples, n_features).\n",
        "\n",
        "Example: y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "LLF_5fbdqEDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20 What are continuous and categorical variables?\n",
        "\n",
        "Ans 20\n",
        "Continuous Variables: Represent measurable quantities. They can take on any value within a range (infinite possibilities).\n",
        "\n",
        "Examples: Height, Weight, Temperature, Income.\n",
        "\n",
        "Categorical Variables: Represent discrete groups or categories. They take on a limited, fixed number of values.\n",
        "\n",
        "Examples: Gender (Male/Female), Product Category (Electronics/Clothing), Country."
      ],
      "metadata": {
        "id": "pQ6Tyr1BqMaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21 What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Ans 21\n",
        "Feature scaling is a preprocessing technique used to standardize the range of independent variables (features) in your data. Since features can have very different units and scales (e.g., age: 0-100, salary: 30,000-200,000), scaling brings them onto a similar scale.\n",
        "\n",
        "Why it helps in Machine Learning:\n",
        "\n",
        "Improves Performance of Distance-Based Algorithms: Algorithms that calculate distances between data points are highly sensitive to the scale of features.\n",
        "\n",
        "Examples: K-Nearest Neighbors (KNN), Support Vector Machines (SVM), K-Means Clustering.\n",
        "\n",
        "Without Scaling: A feature with a larger range (like salary) would dominate the distance calculation, making the algorithm effectively ignore features with smaller ranges (like age).\n",
        "\n",
        "Speeds Up Convergence for Gradient Descent: Models that use gradient descent for optimization (like Linear Regression, Logistic Regression, Neural Networks) converge much faster when features are on a similar scale.\n",
        "\n",
        "Analogy: Imagine trying to walk directly to the bottom of a long, narrow valley. If the valley is stretched along one axis, your path will be zig-zag and slow. Scaling reshapes the valley into more of a bowl, allowing you to take more direct steps to the bottom.\n",
        "\n",
        "Helps Regularization: Regularization techniques (like L1 and L2) penalize large coefficients. If features are not scaled, the penalty would be applied unfairly, affecting features with larger scales more than others.\n",
        "\n",
        "Algorithms that generally NEED scaling: KNN, SVM, Neural Networks, PCA, K-Means, Linear Regression with regularization.\n",
        "Algorithms that generally DO NOT need scaling: Tree-based models (Decision Trees, Random Forest, XGBoost) because they make splits based on feature thresholds, which are scale-invariant."
      ],
      "metadata": {
        "id": "nVOf8qscqW0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22 How do we perform scaling in Python?\n",
        "\n",
        "Ans 22\n",
        "The most common way is using the StandardScaler or MinMaxScaler from the sklearn.preprocessing module.\n",
        "\n",
        "StandardScaler (Z-Score Normalization)\n",
        "\n",
        "Transforms data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Formula: (x - mean) / std\n",
        "\n",
        "Best for: When your data is roughly normally distributed.\n",
        "\n",
        "MinMaxScaler\n",
        "\n",
        "Scales data to a fixed range, typically [0, 1].\n",
        "\n",
        "Formula: (x - min) / (max - min)\n",
        "\n",
        "Best for: When you know the data doesn't follow a normal distribution."
      ],
      "metadata": {
        "id": "wvIvKMS6qxwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23 What is sklearn.preprocessing?\n",
        "\n",
        "Ans 23\n",
        "sklearn.preprocessing is a module in the Scikit-Learn library that provides a wide range of functions and classes for data preprocessing and feature engineering. It's your toolkit for getting raw data ready for machine learning models.\n",
        "\n",
        "Common functionalities include:\n",
        "\n",
        "Scaling: StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "Encoding Categorical Variables: OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
        "\n",
        "Handling Missing Values: SimpleImputer\n",
        "\n",
        "Creating New Features: PolynomialFeatures\n",
        "\n",
        "Custom Transformations: FunctionTransformer\n",
        "\n",
        "It provides a consistent API (like .fit(), .transform()) that works seamlessly with the rest of the Scikit-Learn ecosystem, especially pipelines."
      ],
      "metadata": {
        "id": "uA2KVK1Iq7OZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24 How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Ans 24 The standard method is using the train_test_split function from sklearn.model_selection.\n",
        "\n",
        "Purpose: To create a hold-out set for unbiased evaluation.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X: Features (a DataFrame or 2D array)\n",
        "# y: Target variable (a Series or 1D array)\n",
        "\n",
        "# Split the data: 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y) # Optional, for classification\n",
        "\n",
        "# Parameters:\n",
        "# - test_size: Proportion of the dataset to include in the test split (e.g., 0.2, 0.3)\n",
        "# - random_state: A seed for the random number generator. This ensures the split is reproducible.\n",
        "# - stratify: Very useful for classification. Provides the labels (y) to ensure the train and test sets have the same proportion of classes as the original dataset.\n",
        "\n",
        "\n",
        "Crucial Point: Always split your data BEFORE doing any scaling or imputation to prevent data leakage. The test set should be completely unseen during the training process, including the preprocessing steps."
      ],
      "metadata": {
        "id": "8H9ZBM8SrHWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25 Explain data encoding?\n",
        "\n",
        "Ans 25\n",
        "\n",
        "Data encoding is the process of converting categorical data (text labels) into numerical format that machine learning algorithms can understand. Most algorithms require numerical input.\n",
        "\n",
        "Common Techniques:\n",
        "\n",
        "Label Encoding:\n",
        "\n",
        "What it does: Assigns a unique integer to each category. (e.g., \"Red\"=0, \"Blue\"=1, \"Green\"=2).\n",
        "\n",
        "When to use: For ordinal data where there is a clear order (e.g., \"Low\"=0, \"Medium\"=1, \"High\"=2).\n",
        "\n",
        "Risk with Nominal Data: For categories with no inherent order (like colors), the model might mistakenly think \"Green\" (2) > \"Blue\" (1) > \"Red\" (0), which is meaningless.\n",
        "\n",
        "One-Hot Encoding:\n",
        "\n",
        "What it does: Creates new binary (0/1) columns for each category. The original categorical column is dropped.\n",
        "\n",
        "Original \"Color\" column: [\"Red\", \"Blue\", \"Green\"]\n",
        "\n",
        "After One-Hot Encoding:\n",
        "\n",
        "Color_Red: [1, 0, 0]\n",
        "\n",
        "Color_Blue: [0, 1, 0]\n",
        "\n",
        "Color_Green: [0, 0, 1]\n",
        "\n",
        "When to use: For nominal data where there is no order (e.g., countries, product types).\n",
        "\n",
        "Disadvantage: Can create a very large number of new features if a category has many unique values (high cardinality), which can slow down training."
      ],
      "metadata": {
        "id": "f1m0JVBOrb88"
      }
    }
  ]
}